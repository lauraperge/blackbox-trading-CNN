\documentclass[11pt, a4paper]{article}
\usepackage[english, science, titlepage]{ku-frontpage}
\usepackage[utf8]{inputenc}
\usepackage{booktabs, multirow}
\usepackage{amsfonts, amssymb}
\usepackage{mathtools}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
\usepackage{array}
\newcolumntype{L}{>{\centering\arraybackslash}m{3cm}}

\usepackage{cite, hyperref, nameref}
\usepackage{natbib, apalike, url}
\usepackage{float}

\usepackage{url}
\makeatletter
\g@addto@macro{\UrlBreaks}{\UrlOrds}
\makeatother

\setlength\arraycolsep{2 pt}
\setcounter{tocdepth}{2}
\setcounter{secnumdepth}{0}

\assignment{Master Thesis}
\author{Laura Perge}

\title{Time Series Classification with CNN:}
\subtitle{ Automated Trading by Pattern Recognition}
\date{Handed in: \today}
\advisor{Advisors: Rolf Poulsen, Kenneth H. M. Nielsen, Lasse BÃ¸hling}
%\frontpageimage{example.png}

%% spellcheck-language "en"

\begin{document}
\maketitle

\tableofcontents

\begin{abstract}
    Something.
\end{abstract}

\section{Introduction}
People have been trading on financial markets for more than 200 years and with time, the internet, and technological advancement, the process has changed and evolved into what it is today:  
a versatile, international, online, easy-to-access, automated and truly immense beast. Its evolution is not over, however, and the mentioned features make it just the perfect subject of 
artificial intelligence and machine learning applications. 

But what were some major milestones of this evolution? As nicely summarized by \cite{rialtohistory}  employing algorithms in trading for calculation of asset prices goes back to the beginning of the 20th century. In the early 1950s, Harry Markowitz brought computational finance to existence in the pursuit of portfolio optimization. During that time, the computational 
resources were inadequate to efficiently utilize these algorithms in trading. From the 1970s to 1990s, 
large-scale computerization, the introduction of PCs, the internet and then amongst other systems the ECN (Electronic Communication Network) completely changed the game. Automation became a 
feasible solution, and algorithms are way faster to react with Buy/Sell orders on the market than humans. Trading is now not only for institutions or professionals and a chosen few but for 
every person with access to the internet. Intraday and high-frequency trading emerged and so, using algorithmic trading strategies with automated execution has become crucial in order to 
trade on these markets. 

There is another influential idea that has been around for several decades but has just gained space due to technological progress and the wide-spread access to computational power: artificial intelligence, more 
concisely machine learning and deep learning. 
The first paper about creating a model of the human neural networks was by \cite{mcculloch1943logical} and many more have followed ever since. 
One more achievement related to our topic was the first convolutional neural network (CNN) by \cite{fukushima1979neural} which is the first deep learning model for handwritten character and other 
pattern recognition.

CNN is an exceptionally popular tool nowadays, especially, in the field of image recognition. We recount the details and reasons behind this in the section \nameref{sec:DM}.
The reasons to apply CNN for our problem is supported by both a bottom-up and top-down way. The top-down comes from the recently mentioned recognition of the technique which 
means plenty of resources, established and accurate models together with the ability of feature extraction which comes in handy for time series pattern recognition tasks. 
The bottom-up approach arises naturally from the idea of turning time series into images and label them, so these labels can be assigned to the new images later which, essentially, is 
just an image classification task. 

Generally, when we search for ways of making money by trading on financial markets, the common approach appears to be forecasting time series adopting either a more traditional or a cutting-edge technique. 
Traditional means include autoregressive (AR) and/or moving average models (MA, ARMA) which are limited by the assumed linear relationship between the present and previous values of the univariate time series in question. 
Another limitation is the expectation of stationarity which is solved in the ARIMA (autoregressive integrated moving average) model that still remains linear but takes some preliminary transformation steps 
to turn the problem into a stationary ARMA. Although linearity is mostly acceptable in time series prediction problems, it is not true in all cases, and for that reason  
ARCH and GARCH (\textit{generalized} autoregressive conditional heteroskedasticity) models were invented. These complex models provided solutions to time series analytics which have long been in use and provide explicit insight into the time-dependence structure of the data.\footnote{The interested reader can find more information on these methods in for example Section 2 and 3 of \cite{tsay2005analysis}.}
In the field of AI and deep learning, recurrent neural networks (RNN) is a kind that was engineered to deal with sequential data. To address the issue of remembering long term temporal dependencies long 
short term memory (LSTM) networks were created which are heavily used in time series forecasting problems (\cite{Hochr97LSTM}). 
However, there are plenty of difficulties when it comes to prediction of financial time series. Accurate forecasting with these techniques require a lot of data, moreover, financial time series are many times non-stationary and influenced by regime changes \cite{lemus_2018}.

On the other hand, in this paper, we avert from the application of forecasting the next element of a series and concentrate on predicting trading decisions based on the present prices. We train the model to recognize periods, in the end of which one should make a buying or selling order. 

\textbf{OBJECTIVES: TO BE MODIFIED ACCORDING TO OUTCOME}
The goal of this paper is to develop a simple but powerful model that can make real-time trading decisions utilizing a convolutional neural network. For this, we need to capture as much dynamic 
and static information from the one-dimensional time series as possible which we achieve by turning them into images. The classification exercise requires labels which we choose to represent trading orders of "Buy", "Sell" and "Hold". 
These are preliminarily defined on the training set using a suitable algorithm that is designed to ensure profitability. The ultimate objective is a compact multilevel trading model that takes care of both the creation of image representations and the prediction of financially successful trading strategies using these images, for data that is fed to the model in real time. We also make a contrast between a model trained and used for a specific financial instrument and a "universal" model of multiple assets motivated by the work of \cite{sirignano2018universal}. The expected outcome is that the universal model will give more robustly successful results.

In the section \nameref{sec:RelWork}, we look at the path of publications that motivated and built the foundation of our approach. Then, the section \nameref{sec:DM} provides us with information about the kind of data we use throughout 
the paper, moreover, we introduce the methodologies implemented from the data preparation to the prediction phase. These include the transformation of one-dimensional time series into two-dimensional images, 
how we proceed to create appropriate labels for these images, how we construct and train our CNN model on them and return predictions using the model. 
Afterward, in \nameref{sec:ER} we examine the results and present them in a three-fold manner:
technical fitness (how accurate is the classification?), financial profitability (how high are the returns?), swiftness (how quick is the model to come up with a prediction?). In the section \nameref{sec:Discuss}, an assessment of the presented results is carried out.
The last section is the \nameref{sec:Conclusion} which judges how much 
we closed in on grasping our objectives and what are some possible proceedings of this work.

\section{Related Work}
\label{sec:RelWork}

The overall heuristics of this paper largely resembles the one published by \cite{sezer2018algorithmic}. They propose a deep CNN based algorithmic trading model which they train on financial time series to predict trading orders. The paper diverts from our approach in that it transforms the one dimensional time series into images using 15 different technical indicators over different time periods and fits them into a grid. Their results showed that the performance of their model was quite good against Buy \& Hold and other algorithmic methods even on periods long out of sample. Their conclusion states that they could potentially improve performance by creating more meaningful images.

One potential issue with the image transformation methodology used by the mentioned paper is that the resulting plot does not directly represent the input time series. The image is dependent of the choice of technical indicators and their ordering in the grid. Therefore, we turn to \cite{hatami2018classification} which proposes a similar model but with using Recurrence Plots for transformation; and \cite{wang2015encoding} which uses Markov Transition Fields and Gramian Angular Fields in the different color channels (RGB) of the resulting image. Both methods require a window of elements from the series and maps those to two dimensions according to different formulas which we cover in details in the section \nameref{subsec:DM:TS2IM}.

\section{Data and Methodology}
Overall, 6 models are trained, tuned and tested. There are 5 individual models each exclusively trained and tested on one assigned dataset. This is used to analyze the performance of asset-specific model builds. 
Then, we train the universal model on data coming from 35 financial assets, including the 5 used in the asset-specific models, and test it on these 5 assets once again. This system allows us to compare the performance of the universal model to the asset-specific ones. In order to obtain a more broad assessment, we assess performance on three training and testing time periods but this will be explained in further detail in the section \nameref{subsec:DM:Eval}.

In a real life scenario, the asset-specific models translate to a trader picking certain assets to trade, then gathering all available historical prices for those assets, training and tuning a model for each of them and then use one model per asset to generate trading signals. 
On the other hand, we could imagine a trader gathering data of many assets, including those he wishes to trade, train and tune one universal model on all available data, and use that to generate trading signals for any of the instruments selected for trading. Also, in the second case the trader can start generating trading signals for each of the assets used for training the universal model while in the asset-specific case he needs to collect the historical data of the new asset and build a new model for it.

In the upcoming subsections we will look into what data is used for each of the 6 models, how training, tuning and testing is approached to get comprehensive results, and what other trading techniques we match our performance against.

\label{sec:DM}
\subsection{Data}
\label{subsec:DM:Data}

The 5 asset-specific models are based on the historical adjusted daily closing prices of the following indices, stocks, and ETF:

\begin{itemize}
    \item S\&P 500,
    \item Nikkei 225,
    \item Nasdaq Composite,
    \item Apple Inc.,
    \item SPDR S\&P 500 ETF Trust.
\end{itemize} 

The universal model contains all five assets above and others listed in table \ref{tbl:univ_data}. The assets are selected to represent various asset classes: major stocks, stock indices, exchange traded funds, foreign exchange rates and commodity prices.

\begin{table}[]
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Name}                   & \textbf{Symbol} & \textbf{Dates}          \\ \midrule
\textbf{Main assets}            &                 &                         \\
S\&P500                         & GSPC            & 1950/01/03 - 2019/06/07 \\
Nikkei225                       & N225            & 1965/01/25 - 2019/06/07 \\
Nasdaq                          & IXIC            & 1971/02/05 - 2019/06/07 \\
AAPL                            & AAPL            & 1980/12/12 - 2019/06/07 \\
SPY                             & SPY             & 1993/01/29 - 2019/06/07 \\ \midrule
\textbf{Stock Indices}          &                 &                         \\
DJI                             & DJI             & 1985/01/29 - 2019/06/07 \\
DAX30                           & GDAXI           & 1987/12/30 - 2019/06/07 \\
Shanghai Composite              & SSI             & 1990/12/19 - 2019/06/07 \\
VIX                             & VIX             & 1990/01/02 - 2019/06/05 \\
FTSE100                         & INDEXFTSE: UKX  & 1997/10/20 - 2019/05/31 \\
FTSE250                         & INDEXFTSE: MCX  & 1997/10/20 - 2019/05/31 \\
FTSE350                         & INDEXFTSE: NMX  & 1997/10/20 - 2019/05/31 \\
Eurostoxx 50                    & STOXX50E        & 1986/12/31 - 2019/06/07 \\
Russell 2000                    & RUT             & 1987/09/10 - 2019/06/07 \\ \midrule
\textbf{Exchange Traded Funds}  &                 &                         \\
QQQ                             & QQQ             & 1999/03/10 - 2019/06/07 \\
XLF                             & XLF             & 1998/12/22 - 2019/06/07 \\
XLU                             & XLU             & 1998/12/22 - 2019/06/07 \\
XLP                             & XLP             & 1998/12/22 - 2019/06/07 \\
EWZ                             & EWZ             & 2000/07/14 - 2019/06/07 \\
EWH                             & EWH             & 1996/04/01 - 2019/06/07 \\
XLY                             & XLY             & 1998/12/22 - 2019/06/07 \\
XLE                             & XLE             & 1998/12/22 - 2019/06/07 \\ \midrule
\textbf{Foreign Exchange Rates} &                 &                         \\
GBPUSD                          & DEXUSUK         & 1971/01/04 - 2019/05/31 \\
AUDUSD                          & DEXUSAL         & 1975/01/02 - 2019/05/31 \\
NZDUSD                          & DEXUSNZ         & 1971/01/04 - 2019/05/31 \\
EURUSD                          & DEXUSEU         & 1999/01/04 - 2019/05/31 \\ \midrule
\textbf{Commodities}            &                 &                         \\
Copper                          & Copper          & 1959/07/06 - 2019/06/07 \\
WTI                             & DCOILWTICO      & 1986/01/02 - 2019/06/03 \\
Europe Brent Spot Price         & FOB             & 1987/05/20 - 2019/06/03 \\
Gold                            & XAUUSD          & 1979/12/29 - 2019/06/07 \\
Silver                          & XAGUSD          & 1982/07/02 - 2019/06/07 \\
Platinum                        & Platinum        & 1969/01/02 - 2019/06/07 \\
Corn                            & Corn            & 1959/07/01 - 2019/06/07 \\
Coffee                          & Coffee          & 1973/08/20 - 2019/06/07 \\
Soybean oil                     & Soybean oil     & 1960/10/26 - 2019/06/07
\end{tabular}
\caption{Overview of datasets. The main assets' daily adjusted closing prices are each individually used to train and test asset-specific models, while all of the 35 assets' daily (adjusted) closing prices are used to train the universal model. }
\label{tbl:univ_data}
\end{table}

Each asset's time series take values on business days. One-off missing values are imputed using forward filling which is just propagating the last valid non-missing value to the gaps. Afterwards, each price series is turned into return series. The return $r$ at time $t$ is defined $r_t = \frac{p_t}{p_{t-1}}-1$ where $p_t$ is the price given at any time point $t$.

In terms of scaling, Recurrence Plots and Markov Transition Fields require no scaling steps, while for the Gramian Angular Field transformation, we first use min-max normalization between $[-1, 1]$ according to the general formula for $[a, b]$
\begin{equation}
\label{eq:minmax}
    x^{scaled}_i =(b-a)\frac{x_i-\min(x)}{\max(x) - \min(x)} + a
\end{equation}
given $x = (x_1, \dots, x_n)$ is a univariate time series and $x_i$ is its $i^{th}$ element. Note, that the scaling is done separately on the testing and training data to avoid look-ahead bias.

\subsection{Transformation Strategies: Time Series to Images}
\label{subsec:DM:TS2IM}

We cover the three techniques mentioned before: Recurrence Plots, Markov Transition Fields and Gramian Angular Fields. All three methods are applied on sliding windows of size $s = 30$ (\textbf{TBM!!}) of the different return time series. This means that for $n$ return values we get $(n - s + 1)$ images. 

The \textbf{Recurrence Plots (RP)} originally introduced by \cite{jp1987recurrence} implemented according to \cite{hatami2018classification} are useful when we wish to represent the periodicity of trajectories going through a phase space. The issue with visualization that the RP solves appears when the phase space has more than three dimensions. A recurrence represents the time a trajectory gets back to a previously visited location. In Figure \ref{fig:RP_Def} we use the same explanation presented in Figure 1 of \cite{hatami2018classification}. In the plots it can be observed, how certain values tend to follow each other, for example $x_3 = 1, x_4 = 3$ and then this pattern re-occurs at $x_6, x_7$ which is reflected by $s_3$ and $s_6$ falling to the same phase on the map, and thus by the pixel at $(s_3, s_6)$ being equal to zero.

\begin{figure}[]
    \centering
    \includegraphics[width=\textwidth]{images/RP.PNG}
    \caption{The left plot shows a time series of 7 observations $x_t$, $t=1,\dots,7$. The plot in the middle shows the two dimensional phase space trajectory that is created from $x$ using a time delay of $\tau = 1$. The states are denoted by $s_k$ for $k=1,\dots,13$ where $s_k = (x_k, x_{k+1})$. On the right, the recurrence plot is a matrix of Euclidean distances between the 6 states: $R_{i,j} = Euclidean(s_i,s_j)$.}
    \label{fig:RP_Def}
\end{figure}

After the recurrence plots are created, we scale them to fall in the interval $[0, 1]$ using the appropriate min-max scaling introduced in Equation \ref{eq:minmax} which is a requirement of the CNN input. 

As it is pointed out in \cite{hatami2018classification}, time series tend to show recurrent behaviour like periodicity, and this phenomenon is generally present in dynamic nonlinear systems or stochastic processes that generate the series. The recurrence plot, as described previously via Figure \ref{fig:RP_Def}, reveals which are the phases that trajectories tend to return to. To attain such a plot, each element of it is the result of this formula:
\begin{equation}
\label{eq:RP_R}
    R_{i,j} = \theta(\epsilon-||\vec{s}_i - \vec{s}_j||),\quad \vec{s}(.) \in \mathbb{R}^m, \quad i,j = 1,\dots,K
\end{equation}
where K is the number of considered states $\vec{s}$, $\epsilon$ is a threshold distance, $||.||$ a norm (Euclidean in our case), and $\theta(.)$ the Heaviside function\footnote{The Heaviside function is defined as: $\theta(x) = \frac{d}{dx}\max(0,x)$ for $x \neq 0$}. Summing up the process of creating a recurrence plot for our purposes: first, we start out with a window containing $30$ consequent return values of an asset; then we create the two-dimensional phase space trajectory ($m=2$) from these series which results in $29$ states; finally, the R-matrix is calculated based on Equation \ref{eq:RP_R} but with a small change. The formula, due to the $\epsilon$ threshold parameter, would give us a matrix of ones and zeros, and to avoid this information loss we simply skip this step and work with the resulting grey-scale images. Also, we use min-max scaling on each resulting image of $(29 \times 29)$ to $[0, 1]$ in order to satisfy the requirements of the CNN's input vectors. Afterwards, to fit the image size of the two other transformation strategies introduced later, we add a size $1$ zero-padding to the right and bottom of the image.

% potentially mention RP on prices that increase or decrease similar while on returns it is more distinctive

Another image transformation used is the \textbf{Gramian Angular Field (GAF)} that translates the typical Cartesian coordinates into a polar coordinate system. The technique is introduced in \cite{wang2015encoding}, and is based on the Gramian matrix (named after J{\o}rgen Pedersen Gram), the entries of which are given by $G_{ij} = \langle v_i,v_j \rangle$ for a set of vectors $v_1,\dots, v_n$. These entries are inner products which measure the "similarity" of the two vectors. The Gram matrix is generally used for the calculation of the linear independence of a set of vectors. A beneficial property of the Gram matrix is that it preserves temporal dependency in the geometrical setting of the matrix, time flows from the top left to the bottom-right. For example, the Gramian matrix of a univariate time series: $x_t$ for $t=1, 2, 3$ can be computed as:

$$
G =\begin{pmatrix} 
\langle x_1,x_1 \rangle & \langle x_1,x_2 \rangle & \langle x_1,x_3 \rangle \\
\langle x_2,x_1 \rangle & \langle x_2,x_2 \rangle & \langle x_2,x_3 \rangle \\
\langle x_3,x_1 \rangle & \langle x_3,x_2 \rangle & \langle x_3,x_3 \rangle \\
\end{pmatrix}
$$

As noted in \cite{gaf_medium} the use of the Gramian matrix can be motivated by the fact that plain univariate time series prove unsuccessful in explaining co-occurence and latent states in the data and the Gramian matrix provides an alternative visualization. This same article also shows that the traditional Gram matrix is unsuccessful in making a distinction between the Gaussian noise and the valuable information in the data, thus the entries of the GAF matrix are not simply given by the inner product (in an Euclidean setting). Following the definition in \cite{wang2015encoding}, to create a GAF plot from a time series $X = \{x_1,x_2, \dots x_n\}$ of n valued real observations, we first rescale $X$ using min-max scaling to the $[-1,1]$ interval, as described previously in Equation \ref{eq:minmax}, and end-up with the scaled series $\Tilde{X}$. Now, we transform the time series into polar coordinates:
\begin{align}
\label{eq:gafencode}
    \begin{cases}
        \phi_i = \arccos(\Tilde{x}_i), \quad -1 \leq \Tilde{x}_i \leq 1, \quad \Tilde{x}_i \in \Tilde{X}\\
        r_i = \frac{t_i}{N}, \quad t_i \in \mathbb{N}
    \end{cases}
\end{align}
i.e. we convert the timestamp $t_i$ dividing by $N$ (the number of series instances) and get the radius. As for the angles, they are the angular cosine of the scaled series. Then, we create the Gramian Angular Field by taking the cosine sum between each pair of angles:

\begin{equation}
\label{eq:GAF}
    GAF =\begin{pmatrix} 
\cos(\phi_1 + \phi_1) & \cdots & \cos(\phi_1 + \phi_n)\\
\cos(\phi_2 + \phi_1) & \cdots & \cos(\phi_2 + \phi_n)\\
\vdots & \ddots & \vdots\\
\cos(\phi_n + \phi_1) & \dots & \cos(\phi_n + \phi_n)\\
\end{pmatrix} \\
= \Tilde{X}' \cdot \Tilde{X} - \sqrt{I-\Tilde{X}^2}' \cdot \sqrt{I-\Tilde{X}^2}
\end{equation}
where $I$ is the unit row vector $[1, 1, \dots, 1]$. Notice, that this is a Gramian-like matrix with a penalized inner product given by\footnote{Please see Appendices \nameref{app:GAF} for the proof.}: $\langle x, y\rangle - \sqrt{1-x^2} \cdot \sqrt{1-y^2}= x \cdot y - \sqrt{1-x^2} \cdot \sqrt{1-y^2}$. There are several advantages to this construction one of which is preserving the absolute temporal relations. Another aspect is that the diagonal sustains the original (although transformed) time series. Furthermore, the entries $GAF_{ij}$ of the matrix represent the temporal correlation (cosine-similarity) with respect to the different $k= |i-j|$ time intervals, and the main diagonal displays the case of $k=0$. Figure \ref{fig:GAF} shows how the transformation happens for an example scaled time series of $14$ observations. Time can again be tracked going from the top-left to the bottom-right corner.

\begin{figure}[]
    \centering
    \includegraphics[width=\textwidth]{images/GAF.png}
    \caption{Plot 1 shows the time series scaled to the interval [-1, 1] which is then transformed to polar coordinates. In the polar coordinate system in Plot 2, the first point is the closest to the center, then as time moves forward, the coordinates vary among different angles but move further and further out towards the boundaries of the unit circle. Plot 3 shows the GAF plot that we get by applying the formula in Equation \ref{eq:GAF}. Before applying the CNN, we use min-max scaling to the interval $[0,1]$ to fit the expected input of the model. As Plot 4 demonstrates, this scaling does not change the appearance of the image.}
    \label{fig:GAF}
\end{figure}

% Importantly, the described encoding map in Equation \ref{eq:gafencode} is bijective as $cos(\phi)$ is monotonic given that $\phi \in [0, \pi]$ which IS NOT TRUE HERE, is it? for the single angles yes, but we add them up, which results in non-bijective transformation, for the different input we can get the same output, maybe the radius makes it?

The third and last transformation strategy introduced is called \textbf{Markov Transition Field (MTF)} and it entails encoding dynamic transition probabilities from a Markov Transition Matrix of discretized quantile bins in a quasi-Gramian matrix. The algorithm is introduced in \cite{wang2015encoding} and inspired by \cite{campanharo2011duality}.

In its essence, the method consists of encoding the dynamic transition probabilities in a Markov Transition Matrix and then from that matrix create a new one that also preserves information about the sequentiality of the time series which is the MTF.
To be more precise, given a time series $X_t$ (which, in our case, is the return series) we first identify its $Q$ quantile bins $q_j$ $(j \in [1,Q])$. In Figure \ref{fig:MTF}, Plot 1 illustrates this step. Afterwards, matrix $V$ is a $Q \times Q$ weighted adjacency matrix, with $V_{ij}$ representing the count of transitions from $q_i$ to $q_j$ within one time step. 
%This is also a first-order Markov chain along the time axis.
Then, the Markov Transition Matrix denoted by $W$ is constructed by normalization of $V$:

\begin{equation}
\label{eq:W}
    W_{ij} = V_{ij}/\sum_j V_{ij}.
\end{equation}

Thus, $W_{ij}$ gives us the probability of moving from the $i^{th}$ bin to the $j^{th}$ bin within one time step. Now, the issue with this matrix is that we lose a lot of information and it fails to represent that the data is serial in time. Therefore, \cite{wang2015encoding} suggests a design that sustains the time dependence via the locations of the values in the matrix. The Markov Transition Field can then be described as:

\begin{equation}
    \label{eq:MTF}
        MTF =\begin{pmatrix} 
    W_{ij|x_1 \in q_i, x_1 \in q_j} & \cdots & W_{ij|x_1 \in q_i, x_n \in q_j}\\
    W_{ij|x_2 \in q_i, x_1 \in q_j} & \cdots & W_{ij|x_2 \in q_i, x_n \in q_j}\\
    \vdots & \ddots & \vdots\\
    W_{ij|x_n \in q_i, x_1 \in q_j} & \dots & W_{ij|x_n \in q_i, x_n \in q_j}
    \end{pmatrix}
\end{equation}
and an example can be observed in Plot 3 of Figure \ref{fig:MTF}. This resulting image, similarly to the previous two representations trails the flow of time from the top-left to the bottom-right corner.
The entries of this matrix are already in the range $[0,1]$, so there is no need for further scaling before feeding it to the CNN. The images are created with the same sliding window technique discussed previously. A parameter to decide on is the number of quantile bins to create, and with window size set to $30$ it is set to $5$ in our image construction.\footnote{It should be noted that the number of bins can be optimized via costly hyperparameter optimization but due to limitations in time and computational power this was not carried out in the study.}
The MTF can also be understood as the multispan transition probabilities of the series within one step at different points in time.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{images/MTF.png}
    \caption{Plot 1 shows the time series which is assigned to $Q=4$ quantile bins. In Plot 2.a), we can inspect the preliminary state of the Markov Transition Matrix $V$, the elements of which are the counts of the different transitions in the time series $X$, and in Plot 2.b) $W$ is the so-called right stochastic matrix or Markov Transition Matrix, a real stochastic matrix with transition probabilities between the different bins and each row summing to $1$ (see Equation \ref{eq:W}). Plot 3 shows the MTF plot that we get by applying the formula in Equation \ref{eq:MTF}.}
    \label{fig:MTF}
\end{figure}

Overall, the three representations have some common properties, such as how the time dimension is represented in the way the different pixels are located. Also, all three methods provide us a way to transform sliding windows of time series into graphical representations that an image classifier can potentially recognize given proper labels. However, all three transformations show us a slightly different aspect of the series. While the recurrence plot highlights cyclicality and recurring patterns in the data, the GAF illustrates temporal dependencies and the MTF reveals the transition-related behavior in the series. Figure \ref{fig:allTrf} shows an example of all three transformations on the same return series of length $30$.

\begin{figure}[]
    \centering
    \includegraphics[width=\textwidth]{images/allTrf.PNG}
    \caption{Recurrence Plot, Gramian Angular Field and Markov Transition Field applied on the same return series consisting of 30 observations.}
    \label{fig:allTrf}
\end{figure}

\subsection{Image Labelling}
\label{subsec:DM:IL}
The image labelling is done in two steps. First, we label the original time series and, in a second step, the images using the labels from Step 1.
The labelling algorithm is based on a simple idea and follows the one applied in \textit{Algorithm 1 in} \cite{sezer2018algorithmic}.

The algorithm for the time series is as follows. We choose an odd labelling window size $l = 5$. The oddness is required since we slide this labelling window along the series and label the middle element of the window as "Buy" ("Sell") if the mid point of the window is the minimum (maximum) of the values in the window, otherwise they are "Hold". The process is illustrated in Plots 1-6 in Figure \ref{fig:Labelling}. 
Two special cases are when, after a plunge, the series remains constant and then goes up again, and vice versa. They are also shown in Plots 7-10 in Figure \ref{fig:Labelling}, and the rule is as follows: 
given the middle observation in the labelling window is sharing its property of being a local minimum (maximum) with at least one of its direct neighbours, then the label is a "Buy" ("Sell") only if the observation following the mid point is strictly greater (strictly less) than the mid point.

In Step 2, we digress from the mentioned literature. Each image is labelled as the last element of the series that is used for creating the image. In other words, considering image window size of $s$ and return series $X_t$ for $t = 1, \dots, n$ for $n > s$ which has labels defined by\footnote{The notation $\lfloor \cdot \rfloor$ refers to the floor function.} 
$L_{\bar{t}}$ for 
$\bar{t} = \left(1+\lfloor \frac{l}{2} \rfloor\right), \dots, \left(n-\lfloor \frac{l}{2} \rfloor\right)$ with labelling window size $l$, and each image made up of the subset of the series given by
$X_{(i,\dots,(i+s-1))}$  
for every
$i = \left(1+\lfloor \frac{l}{2} \rfloor\right), \dots, \left(n-s+1-\lfloor \frac{l}{2} \rfloor\right)$ gets the label $L_{i+s-1}$. All in all, this indicates that given a series of $n$ returns, we can label $\left((n - s + 1)-\lfloor \frac{l}{2} \rfloor\right)$ of the $(n - s + 1)$ images created.
There are multiple reasons to pursue this approach. Firstly, if we were to label an image based on the price at its mid point, the prediction of a label would come too late to actually trade at that price. Secondly, the choice of the labelling window size does not have to match the window size used for image creation which is also beneficial because the labelling window size allows us, to some extent, to choose the frequency of "Buy" and "Sell" orders because a smaller window size identifies more of them. 

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{images/Labelling.png}
    \caption{Example of labelling. The figure visualizes the step-by-step labelling of an example series of $12$ observations with labelling window of size $l=5$. The grey rectangles show the scope of values considered in every labelling decision, and that the value in the middle time step is labelled in every step (Plots 1-6). Plots 7-8 show the process in the first special case, that is when the mid value is not the only local minimum, but shares this property with one of its neighbouring observations. The decision in the second special case, depicted in Plots 9-10, is similar, only with two consequent local maxima within the same window. It can also be seen in Plot 11 that the first and last $\lfloor \frac{l}{2} \rfloor = 1$ values cannot be labelled.}
    \label{fig:Labelling}
\end{figure}

\subsection{Recognizing Patterns with Deep CNN}
\label{subsec:DM:RecPatwCNN}
Deep convolutional neural networks (CNN) are artificial neural networks (ANN) with multiple layers between the input and output layers (hence deep) that also employ convolution operation in one or more layers as a substitute for the matrix multiplication (see p.326, Chapter 9 in \cite{goodfellow2016deep}). 
The CNN has many advantageous properties but what is most important for our use-case is that it can process fairly raw high dimensional data and extract the most important features for the given exercise. Moreover, the sparse connectivity in the network prove to be computationally more efficient and better in generalization when compared with multilayer perceptrons (which are fully connected networks). Deep neural networks have many constituents that shall be chosen for the data and objective of the model. Below we will get an overview of these choices and the final model design (also see Figure \ref{fig:CNNstruct}).

The first component that we discuss is the activation function which has an important role in introducing non-linear properties to the neural network. The activation function that is used throughout the models is Leaky ReLU, which is a version of the commonly used ReLU function. ReLU stands for rectified linear unit (\cite{hahnloser2000digital}) and is defined as
\begin{equation}
    \label{eq:ReLU}
    y = f(x) = \max(0,x).
\end{equation}
The ReLU is popular as it prevents against vanishing gradients (\cite{glorot2011deep}) and it has a low computational cost. Lastly, it promotes sparsity since all negative inputs result in zero activation which helps the model in finding meaningful features faster and prevents against overfitting. 
On the other hand, the fact that all negative results are turned into zero can be an issue when a value keeps giving negative results. In this case the ReLU turns it into zero every time and due to not being any slope below zero the neuron is very likely to remain inactive for the rest of the training. This way, these neurons will have no contribution to the network and this is an even bigger problem if more and more neurons turn up "dead". To prevent against the described disadvantages we use Leaky ReLU which solves the issue by introducing a slope in the negative side of the function (\cite{maas2013rectifier}):
\begin{equation}
    \label{eq:LeakyReLU}
    y = f(x) = 
   \begin{cases*}
      x & \text{if $x \geq 0$} \\
      \alpha x        & \text{if $x<0$}
    \end{cases*}
\end{equation}
where we use $\alpha = 0.3$. The Leaky ReLU also helps accelerating the training process as it is more balanced than the simple ReLU. On the output layer we apply softmax activation that is usually used for multiclass classification problems as it normalizes the outputs of the last layer so it becomes a probability distribution over $K$ classes. The standard softmax function is $\sigma: \mathbb{R}^K \rightarrow \mathbb{R}^K$
\begin{equation}
    \label{eq:softmax}
    \sigma(\mathbf{z})_i = \frac{e^{z_i}}{\sum_{j=1}^K e^{z_j}} 
\end{equation}
for $i = 1,\dots, K$ and $\mathbf{z} = (z_1, \dots, z_K) \in \mathbb{R}^K$.\footnote{For more on the softmax activation function please see \cite{nielsen2015neural}}

For regularization, three different methods were implemented: batch normalization, dropout layers and L2 kernel regularization. The batch normalization (\cite{ioffe2015batch}) is both useful to regularize the network and it improves training speed and performance by normalizing the input for each layer in the network for each mini-batch. This is used to avoid the problem of the varying distribution of input during training. The dropout (\cite{srivastava2014dropout}) is a method to reduce overfitting by randomly dropping nodes together with their connections from the network during training with a given drop-probability. The technique works by preventing co-adaptation among units (for more on the topic please see \cite{hinton2012improving}). The third regularization implemented is L2-regularization (or Ridge Regression) which works by penalizing the loss function (\cite{ng2004feature}). This is done by adding the penalty $\frac{1}{2}\lambda w^2$ to the loss function for each weight $w$ with regularization strength $\lambda = 0.01$ in our network. The penalty term helps reducing the role of peaky weights and thus the network pays attention to the nodes in a more balanced fashion.

The loss function is the Kullback-Leibler divergence which, for $n$ prediction vectors $\mathbf{y}^{pred}_i$ ($i= 1, \dots, n$) and $n$ corresponding true value vectors $\mathbf{y}^{true}_i$ ($i= 1, \dots, n$), is given as
\begin{equation}
    \label{eq:KL}
    Loss_{KL} = \sum_{i=1}^{n}\mathbf{y}_i^{true} \log \left( \frac{\mathbf{y}_i^{true}}{\mathbf{y}_i^{pred}}\right)
\end{equation}
and it measures the similarity between the true and predicted distributions.

As optimizer we apply Stochastic gradient descent (SGD) and for the optimization of this algorithm, Nesterov accelerated gradient (NAG). This was chosen by model tuning, generally this setting provided a faster convergence than Adam, Nadam or RMSProp. The SGD update for $w$ parameter at the $k^{th}$ iteration is
\begin{equation}
    \label{eq:SGD}
    w_k = w_{k-1} - \eta_{k-1} \cdot \nabla f(w_{k-1})
\end{equation}
where $\eta$ is the learning rate and $\nabla f(w_{k-1})$ is the stochastic gradient at $w_{k-1}$. With the NAG we break this update into two parts. The Momentum optimization (first introduced by \cite{polyak1964some}) of SGD accelerates the optimization but it also makes it prone to overshooting. The NAG is similar to the Momentum, it also breaks the parameter update into two equations:
\begin{align}
\label{eq:NAG}
    \begin{split}
        v_t &  = \gamma v_{t-1} - \eta_{t-1} \nabla f(w_{t-1} + \gamma v_{t-1}) \\
        w_t & = w_{t-1} + v_t 
    \end{split}
\end{align}
where $\gamma$ is the momentum term determining how much of the previous time step's update vector is represented in the current update vector. In the Momentum method this induces the acceleration to the right direction. Since we use $\gamma v_{t-1}$ to change the $w_t$ parameters, the expression $w_{t-1} + \gamma v_{t-1}$ is an approximation of the next position of the parameter and this is used for calculating the gradient instead of the current parameters (\cite{ruder2016overview}).

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{images/CNNCtrf.png}
    \caption{The basic structure of the Convolutional Neural Network used for trading order prediction}
    \label{fig:CNNstruct}
\end{figure}

The model structure is depicted in Figure \ref{fig:CNNstruct}. For each window of 30 subsequent return values the 3 image representations (each of size $30 \times 30$) are fed to the model through the color channels serving as an input. These are then filtered with $32$ kernels of size $3 \times 3 \times 3$ (using zero padding to maintain image size, and the stride is 1) which gives us 32 convolutional layers of size $3 \times 30 \times 30$. It is worth to note that we use small filters due to our images being also relatively small. The activation map of the layers is obtained by using Leaky ReLU, described previously in Equation \ref{eq:LeakyReLU}, preceded by batch normalization in order to prevent against overfitting. This is further supported by a dropout layer with $0.3$ drop rate \footnote{The drop rate in the first and second dropout layer is $0.3$ for each model built on individual assets but it varies according to tuning in the universal model builds ($0.25$ in certain periods).}. Next, we have 64 convolutional layers of size $3 \times 30 \times 30 \times$ created using filters with the same characteristics as before (including stride and zero padding). This is followed by batch normalization, and then the activation map is once again made using the Leaky ReLU function. After, these convolutional layers are average pooled with $2 \times 2 \times 3$ kernels which results in 64 layers of size $3 \times 15 \times 15$ on which we again apply the Leaky ReLU followed by a second dropout. The information from these layers is condensed by the flattening of the the feature map (output resulting from previous layer) and feeding it to the fully connected layer of size $128$. This is also followed by batch normalization and dropout with $0.5$ drop rate for asset-specific models and $0.3$ or $0.25$ drop rates for the universal models. The decreased dropout is sufficient for the universal asset model due to the larger amount of training data already aiding generalization capabilities. Finally, there is the output layer with softmax activation that gives us the probabilities for each image being "Sell", "Buy" or "Hold" class. The described architecture reflects the basic build of each model that has been trained and tested. Some hyperparameters and further settings were customized in the tuning phase for each model. Such aspects include the class weights (as there is an imbalance between the three classes), mini-batch sizes, dropout rates, L2 regularization strength and the parameters of the optimizer.

\subsection{Evaluation Techniques}
\label{subsec:DM:Eval}

For assessing the technical performance of the CNN, we use accuracy, class-wise and average precision, recall and F1 scores, moreover we will also present the confusion matrix for the different models and testing periods. 
The confusion matrix allows us to observe the absolute number of correctly and wrongly classified observations by classes. 
\begin{figure}[ht]
    \centering
    \includegraphics[width=.5\textwidth]{images/Confusion_matrix.png}
    \caption{The structure of the confusion matrix. The notations "TS", "TB", and "TH" mean True Sell, True Buy and True Hold, while "FS", "FB", and "FH" mean False Sell, Buy, and Hold, the subscript refers to the true class in case of false classification.}
    \label{fig:cmdef}
\end{figure}
The matrix that we will see in the section \nameref{subsec:ER:ClassPerf} has the structure described in Figure \ref{fig:cmdef}.
The accuracy tells us about the proportion of observations that is classified correctly over all the classes $k = \{S, B, H\}$ . Now, given we wish to calculate measurements for a classification on a set of $N$ values, i.e. $N = TS + TB + TH + \sum_k \left(FS_k + FB_k + FH_k\right)$ using the notations in Figure \ref{fig:cmdef}, the accuracy is:
\begin{equation}
    \label{eq:acc}
    Accuracy = \frac{TS + TB + TH}{N}
\end{equation}
the class-wise precision for each class $K$ is given by
\begin{equation}
    \label{eq:pr}
    Precision_K = \frac{TK}{TK +  \sum_k FK_k}
\end{equation}
and the class-wise recall is then 
\begin{equation}
    \label{eq:rec}
    Recall_K = \frac{TK}{TK +  \sum_{k  \smallsetminus \{K\}} Fk_K}
\end{equation}
or in other words, the precision of a class $K$ tells us the proportion of correct classifications among all the predictions of $K$, and the recall gives the percentage of the true $K$ class that is correctly classified as $K$. The F1 score is a useful measurement in unbalanced classification tasks, and it is the harmonic mean of the precision and the recall in every class:
\begin{equation}
    \label{eq:f1}
    F1_K = 2 \frac{Precision_K \cdot Recall_K}{Precision_K + Recall_K}.
\end{equation}
We will also see macro average results over the classes for each of these metrics which means the average of the classwise metric results, e.g. the macro average for the F1-score is $\frac{F1_S + F1_B + F1_H}{3}$.

Furthermore, financial assessment is carried out on the test data which we start by setting an initial capital of $C_0 = 10000$ and a flat transaction based trading commission of $c = 5$ (both in USD). At every time step $t$ the trading orders are executed as per the signals ($s_t$) at price $p_t$ which are predicted by the model for the respective time step.
In case of repeating consequent signals, the first signal prompts a trading action given cash/the asset is available in holdings. The Buy order is completed given money is available (up until the first Buy order, and after each executed Sell) and the Sell order is completed if units of the financial instrument are available for selling (i.e. after each executed Buy), otherwise they are handled as a Hold. In case of a Hold signal at $t$, trade does not take place, only the amount of capital $C_t$ is tracked at the given period.
Assets are assumed to be traded in any increments. Using the described approach over the period $t=1, \dots, T$ the following measurements are calculated for the financial performance evaluation:

\begin{itemize}
    \item number of all executed trades: $n_{trade}$
    \item total profit/loss: 
    \begin{align}
        \label{eq:totPL}
            P\& L = C_T - C_0
    \end{align}
    \item cumulative return: 
    \begin{align}
        \label{eq:cumret}
            r_c = \frac{P \& L}{C_0}
    \end{align}
    \item annualized return (for business year of 260 days): 
    \begin{align}
        \label{eq:annualized}
            r_{ann} = \frac{C_T}{C_0}^{\frac{260}{n}}-1
    \end{align}
    \item average P \& L per trade:
    \begin{align}
        \label{eq:avgpl}
        AvgP \& L = \frac{P \& L}{n_{trade}}
    \end{align}
\end{itemize}

To monitor the performance over different economic regimes, we test the model throughout approximately 2 year periods in 2006-2007, 2009-2010 and May 2017 - June 2019. The models will be trained on all previous available data up to the first day of testing. 
Moreover, the evaluation of competing algorithmic trading techniques, for the same testing periods with the same financial metrics provided in Equations \ref{eq:totPL} - \ref{eq:avgpl}, are carried out to make the evaluation more comprehensive.
There are three methods considered for comparison: the Buy \& Hold, the momentum-based Relative Strength Index (RSI), and a volatility-based strategy using the Bollinger Bands (BB).\footnote{The strategies are described in the Appendices: \nameref{app:CompetingStrats}}

In order to assess feasibility we will also present how much time it takes for the model to train and to return predictions from the trained model.

It is important to note that all presented results in the paper are biased to some extent, since creating unbiased results requires excessive computational resources and time which is not given within the scope of this research. To create unbiased results, each model would need to be run many times with different validation sets. This way, using all results we could assess the overall average performance and via standard deviations we could see how stable/robust the results are. Such detailed assessments can be a part of future researches. In this paper, the mentioned issue is partially overcome by testing on different asset classes and time periods.

\section{Evaluation Results}
\label{sec:ER}

\subsection{Classification Performance}
\label{subsec:ER:ClassPerf}

\begin{table}[H]
\begin{tabular}{l|l|cc|cc|cc|cc}
\multicolumn{1}{m{1cm}|}{\multirow{2}{1cm}{Test period}} & \multicolumn{1}{m{1.5cm}|}{\multirow{2}{1.5cm}{Tested Assets}} & \multicolumn{2}{m{2.5cm}|}{Accuracy} & \multicolumn{2}{m{2.5cm}|}{Average* F1} & \multicolumn{2}{m{3cm}|}{Average* Precision} & \multicolumn{2}{m{2.8cm}}{Average* Recall}  \\
\cline{3-10}
&& CNN-I & CNN-U & CNN-I & CNN-U & CNN-I & CNN-U & CNN-I & CNN-U \\ \hline \hline
\multirow{6}{1cm}{2006-2007} & S\&P500          & 0.51          & 0.45 & 0.36 & 0.45          & 0.36 & 0.43          & 0.35 & 0.46          \\
& Nikkei225        & 0.56          & 0.49 & 0.38 & 0.51          & 0.39 & 0.47          & 0.38 & 0.54          \\
& Nasdaq           & 0.55          & 0.43 & 0.35 & 0.43          & 0.35 & 0.42          & 0.35 & 0.45          \\
& AAPL             & 0.59          & 0.43 & 0.36 & 0.41          & 0.37 & 0.40          & 0.36 & 0.41          \\
& SPY              & 0.57          & 0.44 & 0.38 & 0.47          & 0.38 & 0.44          & 0.38 & 0.50          \\ \cline{2-10}
& \textit{Average} & \textbf{0.55} & 0.45 & 0.37 & \textbf{0.45} & 0.37 & \textbf{0.43} & 0.36 & \textbf{0.47} \\ \hline
\multirow{6}{1cm}{2009-2010} & S\&P500          & 0.59          & 0.44 & 0.40 & 0.41          & 0.40 & 0.39          & 0.40 & 0.44          \\
& Nikkei225        & 0.53          & 0.40 & 0.33 & 0.36          & 0.33 & 0.36          & 0.33 & 0.37          \\
& Nasdaq           & 0.57          & 0.45 & 0.38 & 0.44          & 0.38 & 0.42          & 0.38 & 0.48          \\
& AAPL             & 0.58          & 0.43 & 0.37 & 0.42          & 0.37 & 0.39          & 0.37 & 0.44          \\
& SPY              & 0.58          & 0.44 & 0.38 & 0.43          & 0.38 & 0.41          & 0.38 & 0.45          \\ \cline{2-10}
& \textit{Average} & \textbf{0.57} & 0.43 & 0.37 & \textbf{0.41} & 0.37 & \textbf{0.39} & 0.37 & \textbf{0.44} \\ \hline
\multirow{6}{1cm}{2017-2019} & S\&P500          & 0.59          & 0.44 & 0.38 & 0.52          & 0.39 & 0.48          & 0.38 & 0.57          \\
& Nikkei225        & 0.58          & 0.38 & 0.40 & 0.45          & 0.40 & 0.43          & 0.41 & 0.46          \\
& Nasdaq           & 0.50          & 0.41 & 0.36 & 0.47          & 0.36 & 0.44          & 0.37 & 0.50          \\
& AAPL             & 0.55          & 0.37 & 0.35 & 0.46          & 0.35 & 0.43          & 0.35 & 0.49          \\
& SPY              & 0.56          & 0.42 & 0.37 & 0.49          & 0.36 & 0.45          & 0.37 & 0.53          \\ \cline{2-10}
& \textit{Average} & \textbf{0.56} & 0.40 & 0.37 & \textbf{0.48} & 0.37 & \textbf{0.45} & 0.37 & \textbf{0.51}
\end{tabular}
\caption{Classification results for asset-specific models (CNN-I) and universal models (CNN-U). *Macro average as explained previously.}
\label{tbl:ClassRes}
\end{table}

\subsection{Financial Performance}
\label{subsec:ER:FinPerf}
description of results of Financial Evaluation, tables, text

\begin{table}[H]
\begin{tabular}{l|l|ccccc|ccccc}
\multicolumn{1}{m{1cm}|}{\multirow{2}{1cm}{Test period}} & \multicolumn{1}{m{1.5cm}|}{\multirow{2}{1.5cm}{Tested Assets}} &       \multicolumn{5}{m{3cm}|}{Annual Return} & \multicolumn{5}{m{3cm}}{Number of trades} \\ \cline{3-12}
  &                  &  CNN-I         & CNN-U         & RSI           & BB            & B\&H          & CNN-I            & CNN-U & RSI & BB & B\&H \\ \hline \hline
\multirow{6}{1cm}{2006-2007} & S\&P500       & 0.07          & \textbf{0.13} & 0.06          & 0.08          & 0.07          & 52               & 89    & 7   & 12 & 2    \\
  & Nikkei225        &  0.00          & 0.05          & \textbf{0.08} & 0.07          & -0.03         & 44               & 94    & 9   & 11 & 2    \\
  & Nasdaq       & 0.01          & 0.07          & 0.05          & -0.01         & \textbf{0.08} & 37               & 92    & 7   & 7  & 2    \\
  & AAPL             & 0.27          & 0.18          & 0.32          & 0.35          & \textbf{0.63} & 38               & 72    & 10  & 10 & 2    \\
  & SPY              & 0.06          & \textbf{0.12} & 0.06          & 0.03          & 0.09          & 44               & 94    & 7   & 7  & 2    \\ \cline{2-12}
  & \textit{Average}  & 0.08          & 0.11          & 0.11          & 0.10          & \textbf{0.17} & 43               & 88    & 8   & 9  & 2    \\ \hline
\multirow{6}{1cm}{2009-2010} & S\&P500    & 0.12          & \textbf{0.17} & 0.06          & 0.14          & 0.16          & 32               & 78    & 6   & 6  & 2    \\
  & Nikkei225    & -0.07         & -0.02         & 0.05          & \textbf{0.13} & 0.06          & 38               & 69    & 8   & 12 & 2    \\
  & Nasdaq       & 0.18          & 0.13          & 0.15          & \textbf{0.31} & 0.28          & 45               & 84    & 6   & 12 & 2    \\
  & AAPL        & 0.61          & 0.75          & 0.23          & 0.28          & \textbf{0.85} & 51               & 80    & 6   & 8  & 2    \\
  & SPY           & 0.05          & \textbf{0.22} & 0.07          & 0.08          & 0.19          & 32               & 72    & 6   & 6  & 2    \\ \cline{2-12}
  & \textit{Average} & 0.18          & 0.25          & 0.11          & 0.19          & \textbf{0.31} & 40               & 77    & 6   & 9  & 2    \\ \hline
\multirow{6}{1cm}{2017-2019} & S\&P500       & 0.05          & 0.01          & -0.01         & \textbf{0.11} & 0.08          & 40               & 120   & 5   & 13 & 2    \\
  & Nikkei225     & 0.09          & \textbf{0.11} & 0.05          & 0.00          & 0.02          & 51               & 111   & 9   & 11 & 2    \\
  & Nasdaq         & 0.02          & 0.05          & 0.05          & 0.10          & \textbf{0.11} & 56               & 112   & 7   & 11 & 2    \\
  & AAPL      & 0.07          & 0.09          & \textbf{0.18} & -0.01         & 0.13          & 47               & 114   & 11  & 11 & 2    \\
  & SPY       & 0.04          & 0.06          & 0.00          & \textbf{0.11} & \textbf{0.11} & 49               & 116   & 5   & 13 & 2    \\ \cline{2-12}
  & \textit{Average} & 0.05          & 0.06          & 0.05          & 0.06          & \textbf{0.09} & 49               & 115   & 7   & 12 & 2   
\end{tabular}
\caption{Annual returns and Number of trades for the asset-specific model (CNN-I), the universal model (CNN-U) and the competing strategies: Relative Strength Index (RSI), Bollinger Bands (BB) and Buy\&Hold (B\&H)}
\label{tbl:FinResMain}
\end{table}

\subsection{Time Consumption}
\label{subsec:ER:TimePerf}

\begin{table}[H]
\centering
\begin{tabular}{l|l|cc|cc}
\multicolumn{1}{m{1cm}|}{\multirow{2}{1cm}{Test period}} & \multicolumn{1}{m{1.5cm}|}{\multirow{2}{1.5cm}{Tested Assets}} & \multicolumn{2}{m{3.5cm}}{Training time (mm:ss) } & \multicolumn{2}{m{3.5cm}}{Testing time (s)} \\ \cline{3-6}
&& CNN-I & CNN-U & CNN-I & CNN-U \\ 
\hline \hline
\multirow{6}{1cm}{2006-2007} & S\&P500   & 08:30 & 42:09 & 0.41 & 0.40 \\
          & Nikkei225 & 07:12 & 42:09 & 0.59 & 0.09 \\
          & Nasdaq    & 04:59 & 42:09 & 0.41 & 0.09 \\
          & AAPL      & 10:56 & 42:09 & 0.60 & 0.07 \\
          & SPY       & 03:53 & 42:09 & 0.63 & 0.09 \\ \cline{2-6}
          & \textit{Average}   & 07:06 & 42:09 & 0.53 & 0.15 \\ \hline
\multirow{6}{1cm}{2009-2010} & S\&P500   & 10:11 & 35:15 & 0.30 & 0.39 \\ 
          & Nikkei225 & 05:52 & 35:15 & 0.30 & 0.09 \\
          & Nasdaq    & 04:07 & 35:15 & 0.30 & 0.10 \\
          & AAPL      & 06:35 & 35:15 & 0.68 & 0.09 \\
          & SPY       & 04:56 & 35:15 & 0.61 & 0.10 \\ \cline{2-6}
          & \textit{Average}   & 06:20 & 35:15 & 0.44 & 0.15 \\\hline
\multirow{6}{1cm}{2017-2019} & S\&P500   & 15:33 & 31:00 & 0.33 & 0.31 \\
          & Nikkei225 & 08:34 & 31:00 & 0.30 & 0.06 \\
          & Nasdaq    & 06:41 & 31:00 & 0.29 & 0.08 \\
          & AAPL      & 05:52 & 31:00 & 0.31 & 0.07 \\
          & SPY       & 05:35 & 31:00 & 0.32 & 0.06 \\ \cline{2-6}
          & \textit{Average}   & 08:27 & 31:00 & 0.31 & 0.12
\end{tabular}
\caption{Training and testing times of asset-specific models (CNN-I) and universal models (CNN-U)}
\label{tbl:TimeRes}
\end{table}

\section{Discussion}
\label{sec:Discuss}

\subsection{Competing algorithmic strategies \& the CNN}


\subsection{Asset-specific models \& Universal models}

\subsection{Feasibility \& Constraints}
Overall, looking at Table ??? and the training/prediction times both CNN settings are feasible. Retraining of the model does not take too long, and that can easily fit into the time when the markets are closed. Moreover, in our tests we used a model for two years worth of predictions and the results are still promising (HOPE SO!!!) which suggests that retraining of the model does not have to take place on a daily basis. The prediction time is quick enough even for intraday trading purposes. 

\section{Conclusion}
\label{sec:Conclusion}

Potential ways to go: intraday and higher frequency trading, tuning of labelling and image window size and the entire model. Creating a model based on a combination of RNN/LSTM and CNN (further research how that is feasible in practice).

\section{Appendices}
\label{sec:App}

\subsection{Gramian Angular Field: penalized inner product}
\label{app:GAF}
\begin{equation}
    \begin{split}
        \cos(\phi_1 + \phi_2)  & = \cos(\arccos(x) + \arccos(y)) \\
        & = \cos(\arccos(x)) \cdot \cos(\arccos(y)) - \sin(\arccos(x)) \cdot \sin(\arccos(y))\\
        & = x\cdot y - \sqrt{1-x^2} \cdot \sqrt{1-y^2}\\
        & = \langle x, y \rangle - \sqrt{1-x^2} \cdot \sqrt{1-y^2}
    \end{split}
\end{equation}
 Notice, that the entire operation does not satisfy the assumptions of an inner product (linearity limitation, not necessarily positive definite).

\subsection{Competing Strategies Description}
\label{app:CompetingStrats}

\subsubsection{Buy \& Hold}
The Buy \& Hold strategy is a simple and passive strategy based on buying an asset and keeping it for a long period of time, hoping for long term returns regardless of short term fluctuations on the market.
This will be represented in the testing periods as a Buy order on the first date and a Sell order on the last date with no other trades executed in between.

\subsubsection{Relative Strength Index}
The RSI (\cite{wilder1986relative}) is a momentum oscillator $[0, 100]$ with values over 70 indicating an overbought and below 30 and oversold asset. A Buy order happens when the RSI rises above 30 from below and a Sell is indicated by decreasing below 70 from above. The metric is defined by the two step calculation for a look-back period of $d=20$ days:
\begin{equation}
    \label{eq:RSI1}
    RSI_{step1} = 100 - \left[ \frac{100}{1 + \frac{Avg_d Gain}{Avg_d Loss}}\right]
\end{equation}
where $Avg_d Gain$ and $Avg_d Loss$ are the average percentage gains and/or losses during a look-back period of length $d$. After the first $d$ periods of data is available, the second step is just the smoothing of the step 1 formula:
\begin{equation}
    \label{eq:RSI2}
    RSI_{step2} = 100 - \left[ \frac{100}{1 + \frac{Previous Avg_d Gain \cdot (d-1) + Current Gain}{Previous Avg_d Loss \cdot (d-1) + Current Loss}}\right].
\end{equation}

\subsubsection{Bollinger Bands}
The Bollinger Bands (BB) (\cite{bollinger2002bollinger}) characterize the prices and the volatility of a financial asset. They can be depicted on the chart of an asset as a graphical band showing the envelope maximum and minimum of moving averages over the look- back period $d=20$, and the volatility is represented by the width of the envelope.
If the $d$-period moving average is given by $MA_d$, the $d$-period standard deviation is given by $\sigma_d$ and $c=2$ is a customizable parameter then the bands are:
\begin{equation}
    \label{eq:BBlow}
    lowerBB = MA_d - c \sigma_d
\end{equation}
and
\begin{equation}
    \label{eq:BBup}
    upperBB = MA_d + c \sigma_d.
\end{equation}
The strategy is to Buy when the Adjusted Close price decreases below the $lowerBB$ and Sell if it increases above the $upperBB$.

\subsection{Additional Financial Results}

\begin{table}[H]
\centering
\begin{tabular}{l|l|ccccc}
\multicolumn{1}{m{1cm}|}{\multirow{2}{1cm}{Test period}} & \multicolumn{1}{m{1.5cm}|}{\multirow{2}{1.5cm}{Tested Assets}} &       \multicolumn{5}{m{6cm}}{Cumulative Return}  \\ \cline{3-7}
  &                  &  CNN-I         & CNN-U         & RSI           & BB            & B\&H \\ \hline \hline
\multirow{6}{1cm}{2006-2007}  & S\&P500          & 0.15              & \textbf{0.28} & 0.13          & 0.16          & 0.15          \\
  & Nikkei225        & 0.00              & 0.10          & \textbf{0.16} & 0.13          & -0.07         \\
  & Nasdaq           & 0.01              & \textbf{0.13} & 0.10          & -0.01         & 0.17          \\
  & AAPL             & 0.62              & 0.39          & 0.73          & 0.83          & \textbf{1.64} \\
  & SPY              & 0.13              & \textbf{0.25} & 0.13          & 0.06          & 0.19          \\ \cline{2-7}
  & \textit{Average} & 0.18              & 0.23          & 0.25          & 0.23          & \textbf{0.42} \\ \hline
\multirow{6}{1cm}{2009-2010} & S\&P500          & 0.25              & \textbf{0.37} & 0.12          & 0.30          & 0.35          \\
  & Nikkei225        & -0.14             & -0.05         & 0.10          & \textbf{0.28} & 0.13          \\
  & Nasdaq           & 0.39              & 0.28          & 0.32          & \textbf{0.71} & 0.63          \\
  & AAPL             & 1.60              & 2.05          & 0.52          & 0.64          & \textbf{2.41} \\
  & SPY              & 0.11              & \textbf{0.49} & 0.15          & 0.16          & 0.41          \\ \cline{2-7}
  & \textit{Average} & 0.44              & 0.63          & 0.24          & 0.42          & \textbf{0.79} \\ \hline
\multirow{6}{1cm}{2017-2019} &  S\&P500          & 0.11              & 0.03          & -0.02         & \textbf{0.22} & 0.18          \\
  & Nikkei225        & 0.19              & \textbf{0.23} & 0.10          & 0.01          & 0.03          \\
  & Nasdaq           & 0.03              & 0.11          & 0.11          & 0.22          & \textbf{0.23} \\
  & AAPL             & 0.15              & 0.19          & \textbf{0.40} & -0.03         & 0.27          \\
  & SPY              & 0.09              & 0.12          & -0.01         & \textbf{0.24} & 0.22          \\ \cline{2-7}
  & \textit{Average} & 0.11              & 0.14          & 0.12          & 0.13          & \textbf{0.19}
\end{tabular}
\caption{Cumulative returns for the asset-specific model (CNN-I), the universal model (CNN-U) and the competing strategies: Relative Strength Index (RSI), Bollinger Bands (BB) and Buy\&Hold (B\&H)}
\label{tbl:Cummulative}
\end{table}

\subsection{Data Sources}

The list of sources for all downloaded datasets can be found in Tables \ref{tbl:datasets_main}- \ref{tbl:datasets_commods}.

\begin{table}[H]
    \centering
        \begin{tabular}{p{3.0cm} p{10.0cm}l}
        \hline
        \textbf{Symbol}                                                             & \textbf{Source}                                                                                                                                              & \textbf{Date of Download} \\ \hline
       GSPC                                                                        & \url{https://finance.yahoo.com/quote/\%5EGSPC/history?period1=-630982800\&period2=1559944800\&interval=1d\&filter=history\&frequency=1d}    & June 8, 2019              \\
        N225                                                                        & \url{https://finance.yahoo.com/quote/\%5EN225/history?period1=-157424400\&period2=1559944800\&interval=1d\&filter=history\&frequency=1d}    & June 8, 2019              \\
        IXIC                                                                        & \url{https://finance.yahoo.com/quote/\%5EIXIC/history?period1=34556400\&period2=1559944800\&interval=1d\&filter=history\&frequency=1d}      & June 8, 2019              \\
        AAPL                                                                        & \url{https://finance.yahoo.com/quote/AAPL/history?period1=345423600\&period2=1559944800\&interval=1d\&filter=history\&frequency=1d}         & June 8, 2019              \\
        SPY                                                                         & \url{https://finance.yahoo.com/quote/SPY/history?period1=728262000\&period2=1559944800\&interval=1d\&filter=history\&frequency=1d}          & June 8, 2019              \\ \hline
        \end{tabular}
        \caption{Data sources: Testing datasets}
    \label{tbl:datasets_main}
\end{table}

        \begin{table}[H]
    \centering
        \begin{tabular}{p{3.0cm} p{10.0cm}l}
        \hline
        \textbf{Symbol}                                                             & \textbf{Source}                                                                                                                                              & \textbf{Date of Download} \\ \hline
       
        DJI                                                                         & \url{https://finance.yahoo.com/quote/\%5EDJI/history?period1=-630982800\&period2=1559944800\&interval=1d\&filter=history\&frequency=1d}     & June 8, 2019              \\
        GDAXI                                                                       & \url{https://finance.yahoo.com/quote/\%5EGDAXI/history?period1=567817200\&period2=1559944800\&interval=1d\&filter=history\&frequency=1d}    & June 8, 2019              \\
        SSI                                                                         & \url{https://finance.yahoo.com/quote/000001.SS/history?period1=661561200\&period2=1559944800\&interval=1d\&filter=history\&frequency=1d}    & June 8, 2019              \\
        VIX                                                                         & \url{https://finance.yahoo.com/quote/\%5EVIX/history?period1=631234800\&period2=1559944800\&interval=1d\&filter=history\&frequency=1d}      & June 8, 2019              \\
        INDEXFTSE: UKX                                                              & \url{https://www.londonstockexchange.com/statistics/ftse/ftse.htm}                                                                          & June 8, 2019              \\
        INDEXFTSE: MCX                                                              & \url{https://www.londonstockexchange.com/statistics/ftse/ftse.htm}                                                                          & June 8, 2019              \\
        INDEXFTSE: NMX                                                              & \url{https://www.londonstockexchange.com/statistics/ftse/ftse.htm}                                                                          & June 8, 2019              \\
        STOXX50E                                                                    & \url{https://finance.yahoo.com/quote/\%5ESTOXX50E/history?period1=536367600\&period2=1559944800\&interval=1d\&filter=history\&frequency=1d} & June 8, 2019              \\
        RUT                                                                         & \url{https://finance.yahoo.com/quote/\%5ERUT/history?period1=558223200\&period2=1559944800\&interval=1d\&filter=history\&frequency=1d}      & June 8, 2019              \\ \hline
        \end{tabular}
        \caption{Data sources: Stocks \& Indices}
        \label{tbl:datasets_stocks}
    \end{table}
    
        \begin{table}[H]
    \centering
        \begin{tabular}{p{3.0cm} p{10.0cm}l}
        \hline
        \textbf{Symbol}                                                             & \textbf{Source}                                                                                                                                              & \textbf{Date of Download} \\ \hline
        QQQ                                                                         & \url{https://finance.yahoo.com/quote/QQQ/history?period1=921020400\&period2=1559944800\&interval=1d\&filter=history\&frequency=1d}          & June 8, 2019              \\
        XLF                                                                         & \url{https://finance.yahoo.com/quote/XLF/history?period1=914281200\&period2=1559944800\&interval=1d\&filter=history\&frequency=1d}          & June 8, 2019              \\
        XLU                                                                         & \url{https://finance.yahoo.com/quote/XLU/history?period1=914281200\&period2=1559944800\&interval=1d\&filter=history\&frequency=1d}          & June 8, 2019              \\
        XLP                                                                         & \url{https://finance.yahoo.com/quote/XLP/history?period1=914281200\&period2=1559944800\&interval=1d\&filter=history\&frequency=1d}          & June 8, 2019              \\
        EWZ                                                                         & \url{https://finance.yahoo.com/quote/EWZ/history?period1=963525600\&period2=1559944800\&interval=1d\&filter=history\&frequency=1d}          & June 8, 2019              \\
        EWH                                                                         & \url{https://finance.yahoo.com/quote/EWH/history?period1=828309600\&period2=1559944800\&interval=1d\&filter=history\&frequency=1d}          & June 8, 2019              \\
        XLY                                                                         & \url{https://finance.yahoo.com/quote/XLY/history?period1=914281200\&period2=1559944800\&interval=1d\&filter=history\&frequency=1d}          & June 8, 2019              \\
        XLE                                                                         & \url{https://finance.yahoo.com/quote/XLE/history?period1=914281200\&period2=1559944800\&interval=1d\&filter=history\&frequency=1d}          & June 8, 2019              \\ \hline
        \end{tabular}
        \caption{Data sources: Exchange Traded Funds}
        \label{tbl:datasets_etfs}
    \end{table}
        
        \begin{table}[H]
    \centering
        \begin{tabular}{p{3.0cm} p{10.0cm}l}
        \hline
        \textbf{Symbol}                                                             & \textbf{Source}                                                                                                                                              & \textbf{Date of Download} \\ \hline
        DEXUSUK                                                                     & \url{https://fred.stlouisfed.org/categories/94}                                                                                             & June 8, 2019              \\
        DEXUSAL                                                                     & \url{https://fred.stlouisfed.org/categories/94}                                                                                             & June 8, 2019              \\
        DEXUSNZ                                                                     & \url{https://fred.stlouisfed.org/categories/94}                                                                                             & June 8, 2019              \\
        DEXUSEU                                                                     & \url{https://fred.stlouisfed.org/categories/94}                                                                                             & June 8, 2019              \\ \hline
       \end{tabular}
        \caption{Data sources: Foreign Exchange Rates}
        \label{tbl:datasets_FX}
    \end{table}
    
        \begin{table}[H]
    \centering
        \begin{tabular}{p{3.0cm} p{10.0cm}l}
        \hline
        \textbf{Symbol}                                                             & \textbf{Source}                                                                                                                                              & \textbf{Date of Download} \\ \hline
        
       Copper                                                                      & \url{https://www.macrotrends.net/1476/copper-prices-historical-chart-data}                                                                  & June 8, 2019              \\
        DCOILWTICO                                                                  & \url{https://fred.stlouisfed.org/graph/?g=NPX}                                                                                              & June 8, 2019              \\
        FOB                                                                         & \url{https://www.eia.gov/dnav/pet/hist\_xls/RBRTEd.xls}                                                                                     & June 8, 2019              \\
        XAUUSD                                                                      & \url{https://www.investing.com/currencies/xau-usd-historical-data}                                                                          & June 8, 2019              \\
        XAGUSD                                                                      & \url{https://www.investing.com/currencies/xag-usd-historical-data}                                                                          & June 8, 2019              \\
        Platinum                                                                    & \url{https://www.macrotrends.net/2540/platinum-prices-historical-chart-data}                                                                & June 8, 2019              \\
        Corn                                                                        & \url{https://www.macrotrends.net/2532/corn-prices-historical-chart-data}                                                                    & June 8, 2019              \\
        Coffee                                                                      & \url{https://www.macrotrends.net/2535/coffee-prices-historical-chart-data}                                                                  & June 8, 2019              \\
        Soybean oil                                                                 & \url{https://www.macrotrends.net/2538/soybean-oil-prices-historical-chart-data}                                                             & June 8, 2019              \\ \hline
        \end{tabular}
        \caption{Data sources: Commodities}
        \label{tbl:datasets_commods}
    \end{table}

    

\bibliography{reference}
\bibliographystyle{apalike}

\end{document}
